{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像分類時の前処理まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Subtraction\n",
    "\n",
    "入力画像から平均を引く。各ピクセルから引く。<br>\n",
    "\n",
    "VGG で採用されている。\n",
    "\n",
    "https://arxiv.org/pdf/1409.1556.pdf<br>\n",
    "VGG：2.1 ARCHITECTURE  から抜粋<br>\n",
    "During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image. <br>\n",
    "The only preprocessing we do is **subtracting the mean RGB value**, computed on the training set, \n",
    "from each pixel.The image is passed through a stack of convolutional (conv.) layers, \n",
    "where we use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down,center). \n",
    "\n",
    "### Per-pixel Mean Subtraction\n",
    "\n",
    "入力画像から平均を引く。ピクセル・チャンネルごとに計算された平均を引く。<br>\n",
    "即ち、224x224x3 個の値について個別に平均を計算し用いる。<br>\n",
    "\n",
    "AlexNet、ResNet で採用されている\n",
    "\n",
    "https://arxiv.org/pdf/1512.03385.pdf<br>\n",
    "ResNet：3.4. Implementation から抜粋<br>\n",
    "A 224×224 crop is randomly sampled from an image or its horizontal flip, <br>\n",
    "**with the per-pixel mean subtracted** [21]. The standard color augmentation in [21] is used. \n",
    "\n",
    "### Random Crop\n",
    "\n",
    "画像をランダムにトリミングする<br>\n",
    "\n",
    "AlexNet で採用<br>\n",
    "- 256x256 ピクセルに画像をリサイズし、そこから 224x224 のパッチをランダムに取り出す\n",
    "\n",
    "Chainer の ImageNet サンプルはこれと Horizontal Flip をやっている<br>\n",
    "\n",
    "### Horizontal Flip\n",
    "\n",
    "画像をランダムに水平方向に反転する。\n",
    "\n",
    "ImageNet では縦方向はなく水平方向の flip のみが使われる<br>\n",
    "AlexNet 論文以降必ず使われている模様\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 前処理その2 Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Augmentation\n",
    "\n",
    "まず画像をリサイズする。[256, 480] からランダムに短辺の長さを選ぶ。<br>\n",
    "次に、224x224 のパッチをそこからランダムサンプルする。\n",
    "\n",
    "VGG 論文から採用、ResNet 本家論文でも採用\n",
    "\n",
    "### Aspect Ratio Augmentation\n",
    "\n",
    "上の Scale Augmentation(画像リサイズ) に加え画像のアスペクト比を [3/4, 4/3] で変換する\n",
    "\n",
    "GoogLeNet の論文で使われている。\n",
    "\n",
    "### Color Augmentation\n",
    "\n",
    "各ピクセルの RGB を 3 次元のベクトルの集合だと考え PCA をかける。<br>\n",
    "ガウス分布でノイズを生成し、固有ベクトル方向にノイズを加える。<br>\n",
    "乱数は各ピクセルではなくパッチ全体に対して共通。\n",
    "\n",
    "AlexNet で採用<br>\n",
    "論文によると AlexNet の精度への寄与は 1% 程度。<br>\n",
    "ResNet 本家論文でも使われている。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. テスト時"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "\n",
    "複数のモデルを独立に学習し、それらの予測を平均する。\n",
    "\n",
    "GoogLeNet 論文では、ネットワークは同じだが入力の処理法を変えた 7 つのモデルを<br>\n",
    "アンサンブルしている。<br>\n",
    "single crop だと top-5 error が 2% 弱程度向上。\n",
    "\n",
    "ResNet 本家論文では、34B, 34C, 50, 101, 152×2 の 6 モデルをアンサンブルしている。<br>\n",
    "fully-convolutional-form で top-5 error に 1% 弱程度向上。\n",
    "\n",
    "### 10-crop Testing\n",
    "\n",
    "テスト画像 1 つから data augmentation と類似した手順で 10 個のパッチを切り出し、<br>\n",
    "それぞれに対する予測を平均して答える。\n",
    "\n",
    "AlexNet 論文では、（4 スミ＋中央）×（反転の有無）で 10 個のパッチを切り出している。<br>\n",
    "GoogLeNet は 144 パッチも試してる。<br>\n",
    "ResNet でも採用している。\n",
    "\n",
    "GoogLeNet 論文では、crop 数の精度への影響が載っている。<br>\n",
    "top-5 error で、10 crops で約 1% 弱向上、<br>\n",
    "144 crops で 2% 強向上（下表は GoogLeNet 論文より引用）。\n",
    "\n",
    "<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/i/iwiwi/20161231/20161231213228.png\"\n",
    "width=\"380px\" height=\"380px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-convolutional-form Testing\n",
    "\n",
    "まず、全結合層たちを畳み込み層とみなす。\n",
    "\n",
    "例えば、直前の画像サイズが s×s であれば、最初の全結合層は s×s → 1×1 の畳み込み層であるとみなす。<br>\n",
    "すると、ネットワークは全レイヤーが畳み込みとなる。fully convolutional なネットワークの利点は、<br>\n",
    "入力の画像サイズが変化しても適用することができることである（ただし、出力サイズも変化する）。\n",
    "\n",
    "テスト画像をリサイズする。この時の画像サイズは、学習で使っている画像サイズと一致しているとは限らないし、\n",
    "正方形とも限らない。\n",
    "\n",
    "例えば、短辺を 480 にする。ネットワークをこの画像に適用する。出力を average pooling してそれを予測とする。\n",
    "\n",
    "さらに、テスト画像のサイズを複数試し、その結果の平均を用いる。\n",
    "\n",
    "例えば、ResNet 本家論文では短辺を 224, 256, 384, 480, 640 になるようにリサイズしている。<br>\n",
    "また、horizontal flip も試して平均を取る。\n",
    "\n",
    "VGG から使われている。<br>\n",
    "ResNet 本家論文を見るところ、10-crop Testing と比べて、大体 2% 程度精度に寄与している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] VGG‒Net (2014)<br>\n",
    "Very Deep Convolutional Networks for Large‒Scale Visual Recognition. arXiv: 1409.1556, 2014. <br>\n",
    "https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "[2]GoogleNet(2014)<br>\n",
    "Going deeper with convolutions. arXiv: 1409.4842, 2014. <br>\n",
    "https://arxiv.org/pdf/1409.4842.pdf\n",
    "\n",
    "[3]ResNet(2015)<br>\n",
    "Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015.<br>\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "[4]CNN による画像分類で使われる前処理・テスト時処理まとめ<br>\n",
    "http://iwiwi.hatenadiary.jp/entry/2016/12/31/162059\n",
    "\n",
    "[5]画像の水増し方法をTensorFlowのコードから学ぶ<br>\n",
    "https://qiita.com/Hironsan/items/e20d0c01c95cb2e08b94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
