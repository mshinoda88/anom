{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文輪読会＃4　VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 論文を読む際に注意すべき観点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- どんなもの？\n",
    "- 先行研究と比べてどこがすごい？\n",
    "- 技術や手法のキモはどこ？\n",
    "- どうやって有効だと検証した？\n",
    "- 議論はある？\n",
    "- 次に読むべき論文は？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 技術のキモ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込み層と全結合層（計16層）を含むCNN。\n",
    "\n",
    "畳み込み層の下の数字は畳み込みフィルタ数を表す。畳み込みフィルタの大きさは全て３×３。<br>\n",
    "全結合層は、4096ユニット2層＋クラス分類用の1000ユニット1層からなる。\n",
    "\n",
    "<img src=\"./15_imgnet/img500.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 論文精読"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image. \n",
    "\n",
    "トレーニング中に、ConvNetsへの入力は固定サイズの224×224のRGB画像\n",
    "\n",
    "The only preprocessing we do is **subtracting the mean RGB value**, computed on the training set, from each pixel.\n",
    "\n",
    "前処理として「Mean Subtraction」を採用<br>\n",
    "唯一の前処理トレーニングセットで計算された平均RGB値を各ピクセルから減算\n",
    "\n",
    "The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down,center). \n",
    "\n",
    "3×3の非常に小さなフィルタで畳み込み\n",
    "\n",
    "In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). \n",
    "\n",
    "1×1畳み込みフィルタも使用<br>\n",
    "入力チャネルの線形変換（非線形性が続く）と見ることができる\n",
    "\n",
    "The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. \n",
    "\n",
    "パディング1, ストライドは1ピクセルに固定で変換後の解像度保持\n",
    "\n",
    "Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). \n",
    "\n",
    "プーリング層は５つでMaxプーリング\n",
    "\n",
    "Max-pooling is performed over a 2 × 2 pixel window, with stride 2.\n",
    "\n",
    "Maxプーリングは、ストライド2\n",
    "\n",
    "A stack of convolutional layers (which has a different depth in different architectures) is followed by\n",
    "three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-\n",
    "way ILSVRC classification and thus contains 1000 channels (one for each class). \n",
    "\n",
    "3つの全結合層,最初の2つはそれぞれ4096チャネル、3番目のレイヤーは1000チャネル\n",
    "\n",
    "The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks.\n",
    "All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity.\n",
    "\n",
    "最後の層はsoft-max, 活性化関数はReLU\n",
    "\n",
    "We note that none of our networks (except for one) contain Local Response Normalisation(LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, \n",
    "\n",
    "ネットワークには、Local Response Normalization（LRN）正規化を含んでいない\n",
    "\n",
    "such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al., 2012).\n",
    "\n",
    "この正規化は\n",
    "\n",
    "- ILSVRCデータセットの性能を改善しないが、\n",
    "- メモリ消費および計算時間を増加させる。\n",
    "\n",
    "該当する場合、LRN層のパラメータは（Krizhevsky et al。、2012）のパラメータである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column. \n",
    "In the following we will refer to the nets by their names (A–E). \n",
    "\n",
    "All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A(8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). \n",
    "\n",
    "ネットワークA（8畳み込み層および3 全結合層）の11個のレイヤから<br>\n",
    "ネットワークE（16個の畳み込み層および3つの全結合層）の19個のレイヤまで、深さのみが異なる。\n",
    "\n",
    "The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.\n",
    "\n",
    "畳み込み層（チャネル数）は、第1層の64から始めて、各最大プール層の後で2倍に増加し、<br>\n",
    "512に達するまではかなり小さい。\n",
    "\n",
    "In Table 2 we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in (Sermanet et al., 2014)).\n",
    "↓\n",
    "大きな深度にもかかわらず、ネットの重みの数は、より大きな畳み込み層を持つより<br>\n",
    "浅いネットの重みの数よりも大きくない."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 局所的な結合性（Local Connectivity）\n",
    "\n",
    "画像のような巨大な入力のときには、全ニューロンが全ニューロンと結合するのは実際的ではない.\n",
    "\n",
    "その代わりに**局所的に結合し合う**. これをニューロンのreceptive field(受容野)という。<br>\n",
    "(=fileter size)\n",
    "\n",
    "例1<br>\n",
    "[32 * 32 * 3]のサイズの入力のとき<br>\n",
    "receptive fieldが[5 * 5]とすると、ConvLayerは[5 * 5 * 3]の重みをもつ。<br>\n",
    "\n",
    "例2<br>\n",
    "[16 * 16 * 20]のサイズの入力のとき<br>\n",
    "receptive fieldが[3 * 3]とすると、ConvLayerは[3 * 3 * 20]の重みをもつ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 DISCUSSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al., 2014). \n",
    "\n",
    "私たちのConvNet構成は、ILSVRC-2012/ILSVRC-2013競技会のパフォーマンスの<br>\n",
    "高いエントリで使用された構成とは大きく異なる。\n",
    "\n",
    "Rather than using relatively large receptive fields in the first conv. layers(e.g. 11×11 with stride 4 in (Krizhevsky et al., 2012), or 7×7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). \n",
    "\n",
    "It is easy to see that a stack of two 3×3 conv. layers (without spatial pooling in between) has an effective receptive field of 5×5; threenTable 1: ConvNet configurations (shown in columns). \n",
    "\n",
    "The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). \n",
    "\n",
    "The convolutional layer parameters are denoted as “convhreceptive field sizei-hnumber of channelsi”.\n",
    "The ReLU activation function is not shown for brevity.\n",
    "\n",
    "最初のコンバージョンで比較的大きな受容野を使用するのではなく、<br> \n",
    "ストライド11×11、ストライド2のストライド7×7、<br>\n",
    "非常に小さい3×3すべてのピクセル（入力ストライド1）で入力とコンボリューションされたネット全体の受容野。\n",
    "\n",
    "3×3の畳み込み層2つのスタックを見るのは簡単.<br>\n",
    "その間に空間的プールを有さない<br>\n",
    "5×5の有効受容野を有する\n",
    "\n",
    "より多くのレイヤーが追加されるにつれて、構成の深さは左（A）から右（E）に<br>\n",
    "増加する（追加されたレイヤーは太字で示されている）。\n",
    "\n",
    "畳み込みレイヤパラメータは、「畳み込みフィールドサイズ - チャネル数i」と表される。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "such layers have a 7 × 7 effective receptive field. So what have we gained by using, for instance, a\n",
    "stack of three 3×3 conv. layers instead of a single 7×7 layer? First, we incorporate three non-linear\n",
    "rectification layers instead of a single one, which makes the decision function more discriminative.\n",
    "Second, we decrease the number of parameters: assuming that both the input and the output of a\n",
    "three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by 3*(3^2*C^2)\u0001= 27C^2\n",
    "weights; at the same time, a single 7 × 7 conv. \n",
    "\n",
    "layer would require 7^2*C^2 = 49*C^2 parameters, i.e.\n",
    "81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non linearity injected in between).\n",
    "\n",
    "The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the nonlinearity of the decision function without affecting the receptive fields of the conv. layers. \n",
    "\n",
    "Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is\n",
    "introduced by the rectification function.\n",
    "\n",
    "It should be noted that 1×1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).\n",
    "\n",
    "Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. \n",
    "\n",
    "Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better \n",
    "performance.\n",
    "\n",
    "そのような層は有効な7×7受容野を有する。それで、例えば、aを使って得たものは何ですか？\n",
    "3×3コンバインのスタック。レイヤーを1つの7×7レイヤーの代わりに使用しますか？まず、3つの非線形\n",
    "単一のものの代わりに整流層を使用することができ、決定機能をより区別しやすくする。\n",
    "第2に、パラメータの数を減らす。すなわち、aの入力と出力の両方が\n",
    "3層3×3畳み込みスタックはCチャネルを有し、スタックは3 *（3 ^ 2 * C ^ 2）= 27C ^ 2によってパラメータ化される\n",
    "重量;同時に、単一の7×7のコンバージョンが達成されます。\n",
    "\n",
    "層は、7 ^ 2 * C ^ 2 = 49 * C ^ 2のパラメータを必要とする。\n",
    "81％以上。これは、7×7のコンバージョンに正則化を課すと見なすことができます。 3×3フィルタ（その間に非線形性が注入されている）を介して分解されるように強制する。\n",
    "\n",
    "1×1コンバージョンの組み込み層（構成C、表1）は、convの受容野に影響を与えることなく、決定関数の非線形性を増加させる方法である。層。\n",
    "\n",
    "我々の場合、1×1畳み込みは、本質的に同じ次元の空間への線形投影（入力チャネルと出力チャネルの数は同じです）ですが、追加の非直線性は\n",
    "整流機能によって導入される。\n",
    "\n",
    "1×1 convは、層は最近Linらの \"Network in Network\"アーキテクチャで利用されている。 （2014）。\n",
    "\n",
    "小型の畳み込みフィルタはCiresan et al。 （2011年）、それらのネットは私たちのものよりもかなり深くはなく、大規模なILSVRCデータセットでは評価されませんでした。\n",
    "\n",
    "Goodfellow et al。 （2014年）は、道路番号認識のタスクに深いConvNets（11重量層）を適用し、深さの増加がより良い\n",
    "パフォーマンス。\n",
    "\n",
    "ILSVRC-2014分類タスクの成果を上げているGoogLeNet（Szegedy et al。、2014）は、私たちの研究とは独立して開発されましたが、非常に深いConvNets\n",
    "\n",
    "GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prismaの背景技術(Neural Style Transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prisma<br>\n",
    "http://prisma-ai.com/\n",
    "\n",
    "写真をピカソやゴッホのようなスタイルに変換できるアプリ\n",
    "\n",
    "ディープラーニングを使ったアート系の論文は色々と出ていますが、一番基礎となる論文はこれ<br>\n",
    "Image Style Transfer Using Convolutional Neural Networks<br>\n",
    "https://goo.gl/9FCnFy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基礎理論\n",
    "\n",
    "<img src=\"./15_imgnet/img501.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルは、畳み込みニューラルネットワーク（CNN）を使用していて、VGG (2014のILSVRCというコンペで優勝したモデル)\n",
    "がベース\n",
    "\n",
    "このモデルは画像分類（image classification）用に訓練されています。\n",
    "\n",
    "VGG19とVGG16で畳み込み層の数がちょっと違いますが、<br>\n",
    "以下のような構成になっています。論文ではVGG19の方を使っています<br>\n",
    "（どちらを使っても結果はあまり変わらないそうです）。\n",
    "\n",
    "<img src=\"./15_imgnet/img502.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スタイル変換には、このVGGから全結合層を取り除いたものを使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./15_imgnet/img503.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上段：スタイル 下段：コンテンツ<br>\n",
    "a, b, c, d, eはそれぞれconv1_2, conv2_2, conv3_2, conv4_2, conv5_2に対応\n",
    "\n",
    "下段について<br>\n",
    "画像はそれぞれの層において、入力画像を復元したもの.<br>\n",
    "a, b, cまでは元の入力画像とほとんど変わらないように見えますが、<br>\n",
    "d, eでは詳細な情報が落ちてきているように見えます。\n",
    "\n",
    "VGGは元々画像を分類する目的で訓練されていて、**深い層に行くにつれて、分類するにあたって<br>\n",
    "重要なコンテンツが残り、それとはあまり関係のない詳細な見た目などの情報は落ちていっている**<br>\n",
    "と考えられます。\n",
    "\n",
    "この性質を生かして、コンテンツとスタイルをある程度分離することができている<br>\n",
    "とも考えられる。\n",
    "\n",
    "CNNによるコンテンツとスタイルの分離がこの論文の重要な貢献となっていて、<br>\n",
    "この性質をうまく利用し、**コンテンツを保ったままスタイルを別のものと入れ替える**ことを考えます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "\n",
    "コンテンツの損失＋スタイルの損失\n",
    "\n",
    "を損失関数として最小化する.\n",
    "\n",
    "コンテンツの損失は、conv4_2において、コンテンツ画像と生成画像を比較する\n",
    "\n",
    "$$\\mathcal{L}_\\text{content}(\\vec{p}, \\vec{x}, l) = \\frac{1}{2} \\sum_{i, j} (F^{l}_{ij} - P^{l}_{ij})^2$$\n",
    "\n",
    "$\\vec{p}, \\vec{x}$はそれぞれ元のコンテンツ画像と生成画像を表す.\n",
    "\n",
    "l層におけるフィルタ数（特徴マップ数）を$N_l$、特徴マップのサイズ（幅x高さ）を$M_l$とすると、<br>\n",
    "$F^{l} \\in \\mathcal{R}^{N_l \\times M_l}$の関係があります。<br>\n",
    "$F^l_{ij}$は、$i$番目のフィルタによる位置$j$における**活性度**を表します。<br>\n",
    "それぞれの場所における活性度の違いの総和を取っている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スタイルの損失\n",
    "\n",
    "$$G^l_{ij} = \\sum_k F^l_{ik} F^l_{jk}$$\n",
    "\n",
    "この$G^l \\in \\mathcal{R}^{N_l \\times N_l}$は、グラム行列と呼ばれるもの.\n",
    "これをスタイル画像と生成画像で比較する。\n",
    "\n",
    "$$E_l = \\frac{1}{4 N^2_l M^2_l} \\sum_{i,j} (G^l_{ij}-A^l_{ij})^2$$\n",
    "\n",
    "$A_l$ はスタイル画像の方のグラム行列.<br>\n",
    "スタイルについてはある一つ層を考えるだけでなく、**複数の層を考慮**\n",
    "\n",
    "-> スタイルの損失は以下で表現される\n",
    "\n",
    "$$\\mathcal{L}_\\text{style} (\\vec{a}, \\vec{x}) = \\sum^L_{l=0} w_l E_l$$\n",
    "\n",
    "ここで$\\vec{a}$はスタイル画像を表し、$w_l$は各層の損失の重みを表す.<br>\n",
    "(論文ではconv1_1、conv2_1、conv3_1、conv4_1、conv5_1を使用)\n",
    "\n",
    "トータルの損失\n",
    "\n",
    "$$\\mathcal{L}_\\text{total}(\\vec{p}, \\vec{a}, \\vec{x})\n",
    "= \\alpha \\mathcal{L}_\\text{content}(\\vec{p}, \\vec{x})\n",
    "+ \\beta \\mathcal{L}_\\text{style} (\\vec{a}, \\vec{x})\n",
    "$$\n",
    "\n",
    "$\\alpha$と$\\beta$はコンテンツとスタイルの損失のそれぞれの重み.\n",
    "\n",
    "計算の流れ\n",
    "\n",
    "<img src=\"./15_imgnet/img504.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化\n",
    "\n",
    "通常は入力が固定で重みが更新されていきます.<br>\n",
    "今回は逆で**重みが固定で入力画像が更新されていく**ことに注意\n",
    "\n",
    "つまり、$\\frac{\\partial \\mathcal{L}_\\text{total}}{\\partial \\vec{x}}$ を計算することになる.<br>\n",
    "最適化はいつものようにAdamなどを使ってもよいのですが、論文によると**L-BFGSで一番よい結果が得られた**とのこと\n",
    "\n",
    "L-BFGS<br>\n",
    "http://kotarotanahashi.github.io/blog/2015/10/03/l-bfgsfalseshi-zu-mi/\n",
    "\n",
    "通常はSGDのように一次の勾配が使われることが多い.今回は二次の勾配を利用する.<br>\n",
    "一次の勾配は直線的.二次の勾配では曲率を考慮する<br>\n",
    "\n",
    "<b>二次の勾配を使うメリット</b><br>\n",
    "- 学習係数のようなハイパーパラメータを設定する必要がない<br>\n",
    "直線的な場合は、どれくらい移動するか学習係数を使って決めてやる必要がある<br>\n",
    "- 曲線的な場合はおわんの底のような場所に移動してしまえばいい\n",
    "\n",
    "<b>ニュートン法の問題点</b><br>\n",
    "二次の勾配ニュートン法が代表的.<br>\n",
    "二次の勾配を扱うためにヘシアン（Hessian）という行列が必要になる<br>\n",
    "例:100万個のパラメータがある場合は、100万x100万の行列になり、メモリに載らない\n",
    "\n",
    "<b>準ニュートン法を使うわけ</b><br>\n",
    "メモリを節約できるようにしたのが、L-BFGS\n",
    "\n",
    "- ミニバッチのようにノイズがある場合はうまくいかない -> フルバッチが可能な場合に使用が限られる\n",
    "- 小規模な計算なので、L-BFGSを使える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>メモ</b>\n",
    "\n",
    "（１）論文では、VGG19を利用していますが、それよりも少し小さいVGG16を使用でOK<br>\n",
    "VGG19でもVGG16でも結果はほとんど変わらなかったという報告があります\n",
    "\n",
    "https://github.com/keras-team/keras/pull/3347\n",
    "\n",
    "**The results from VGG-19 are very similar to that of VGG-16**. Upon visual inspection, I cannot differentiate between image created by using VGG-19 or by VGG-16. Perhaps I need to run a few tests with 1000 epochs for VGG-19 and 16 to see if I can differentiate between them.\n",
    "\n",
    "\n",
    "（２）論文ではMaxプーリングより、平均プーリングの方が僅かに良い結果が得られたとのこと<br>\n",
    "2. Deep image representations<br>\n",
    "For image synthesis we found that replacing the maximum pooling operation by average pooling yields slightly more\n",
    "appealing results, which is why the images shown were generated with average pooling.\n",
    "\n",
    "画像合成では、最大プーリング操作を平均プーリングで置き換えると、若干よい結果が得られた.<br>\n",
    "理由は、表示された画像が平均プーリングで生成されたから\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スタイル変更の適用結果\n",
    "\n",
    "<img src=\"./15_imgnet/img505.png\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 変更すると、スタイルが強すぎたのが抑えられ、コンテンツがはっきりと分かるようになる\n",
    "- 損失の重みは結果に大きく影響する\n",
    "- イテレーションの回数を増やしたりすれば、もっと良い結果が得られる可能性はあり\n",
    "- 画像によって最適なパラメータは結構違いそう\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION<br>\n",
    "https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "[2] prisma<br>\n",
    "http://prisma-ai.com/\n",
    "\n",
    "[3] Image Style Transfer Using Convolutional Neural Networks<br>\n",
    "https://goo.gl/9FCnFy\n",
    "\n",
    "[4] L-BFGS<br>\n",
    "http://kotarotanahashi.github.io/blog/2015/10/03/l-bfgsfalseshi-zu-mi/\n",
    "\n",
    "[5] L-BFGS法はだからメモリが節約できるのか！<br>\n",
    "https://abicky.net/2010/06/22/114613/\n",
    "\n",
    "[6] Keras 実装例：Newral Style Transfer<br>\n",
    "https://github.com/keras-team/keras/blob/master/examples/neural_style_transfer.py\n",
    "\n",
    "[7] update examples/neural_style_transfer.py<br>\n",
    "https://github.com/keras-team/keras/pull/3347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
